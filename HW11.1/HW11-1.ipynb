{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b824578-57ba-464c-8be4-774ec95fa451",
   "metadata": {},
   "source": [
    "# HW11.1 Fine-tuning BERT LLM using Huggingface Transformers library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e557e4b-7bff-4f7a-be8d-970902f59c46",
   "metadata": {},
   "source": [
    "In this homework, we will step away from tensorflow keras for a moment and instead use the Transformers library from HuggingFace (https://huggingface.co/) . The HuggingFace is a community that hosts pre-trained models from LLMs to computer vision and audio ML models. You can gain easy access to SOTA LLMs using their `transformers` library, fine tuning them, and use standard benchmark datasets from their `datasets` library (it is a generic name but the library is called datasets). \n",
    "\n",
    "Specifically what you will do in this home work:\n",
    "1. Walk through the example of loading the `sst2` dataset (Stanford Sentiment Treebank dataset, essentially a dataset for sentiment analysis) from the `GLUE` benchmark we talked about in class. The GLUE covers a range of NLP tasks and is used to benchmark LLMs. After you load the dataset, there will be some example usages to inspect the dataset.\n",
    "2. From the `transformers` library, load the pretrained LLM called DistillBERT, a variant and smaller version of the famous BERT LLM.\n",
    "3. Fine tune (train further) the DistillBERT model on the `sst2` dataset to achieve a better performance.\n",
    "4. Evaluate your fine-tuned model on `sst2` and compare that with: (1)the model before fine-tuning; (2) the default model in the HuggingFace library that is fine tuned by experts.\n",
    "\n",
    "Please complete all tasks/code and answer all questions. \n",
    "\n",
    "## Requirements\n",
    "\n",
    "You will need the following libraries at the minimum: \n",
    "\n",
    "```\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install accelerate -U\n",
    "!pip install torchinfo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2197e6-6b1a-4873-b354-6a1f15ad5baa",
   "metadata": {},
   "source": [
    "# 1. Load SST2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60f22ce-e0d8-4e61-88f5-f8820844277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# to view the GLUE - SST2 data set and what it is about, see: https://huggingface.co/datasets/nyu-mll/glue\n",
    "# essnentially this is a Stanford Sentiment Treebank dataset for sentiment analysis\n",
    "datasets = load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05faf271-5f67-45ef-812e-64146708cfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can inspect this dataset and see what it contains\n",
    "# you will see it has been divided into three parts: train, val, and test\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e7212-100a-4bc8-a74e-663760da10d1",
   "metadata": {},
   "source": [
    "## Task 1: inspect data text and labels \n",
    "\n",
    "what are the labels? what does label 0 and 1 represent? Take a note of the keys in this dictionary and their values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb4d23d-1c77-4046-a823-3fb2c4d70b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: inspect the first three examples in the datasets\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92ef12-c637-4312-95bb-6b022ac19ee8",
   "metadata": {},
   "source": [
    "# 2. Load pre-trained model DistillBERT and preprocess text\n",
    "\n",
    "We've talked about how each LLM comes with its on (subword, learned) tokenizer. Here, when we load the pre-trained LLM, we also load its tokanizer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea285397-ab7a-47da-897e-6e4b0f82557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences = tokenizer(datasets['train'][:3]['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca429c-ae4b-49e1-a9b8-856efd570e5e",
   "metadata": {},
   "source": [
    "## Task 2: understand what tokenizer is doing\n",
    "Now we've used the tokenizer to tokenize the first three sentences in train dataset. Inspect the tokenized sentences. Let's take the first sentence. It is now represented by a sequences of integer indexes. Can you map them back to actual sub-word units to see how the tokenizer is breaking up the words? \n",
    "\n",
    "Hint: you can do `dir(tokenizer)` to find out how to convert ids to tokens. This applies to any object in python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "608337e8-677f-40b8-b3ca-ebaf17f68f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd9b44-9489-4b58-acfb-f91c4d965e35",
   "metadata": {},
   "source": [
    "The following function applies the tokenizer to all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfe79166-dbbe-4404-bc3b-95af0e11d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(batch):\n",
    "  return tokenizer(batch['sentence'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7321b54-73e6-425c-bb11-c7510fcaec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08580c-b8e6-4d18-9f78-c4c9e03c3eae",
   "metadata": {},
   "source": [
    "# 3. Fine-tune the pre-trained DistillBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "740e1682-16d1-456d-b0b2-55cfeba19d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b4475cb-0f46-497b-9c18-e85d41a66009",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  'my_trainer',\n",
    "  evaluation_strategy='epoch',\n",
    "  save_strategy='epoch',\n",
    "  num_train_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "579ee427-2e85-4bd5-81d0-22180f93808e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89e7801d-aaf1-413a-8f47-7b96095c4e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this warning above tells you that this pretrained model was topped with a newly \n",
    "# initialized classifier that needs to be trained/fine-tuned\n",
    "# let's inspect this model and understand its internal structure\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9918d2a1-f238-40a8-8220-29669584b72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForSequenceClassification                     --\n",
       "├─DistilBertModel: 1-1                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              23,440,896\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─LayerNorm: 3-3                              1,536\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             42,527,232\n",
       "├─Linear: 1-2                                           590,592\n",
       "├─Linear: 1-3                                           1,538\n",
       "├─Dropout: 1-4                                          --\n",
       "================================================================================\n",
       "Total params: 66,955,010\n",
       "Trainable params: 66,955,010\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "# another way to inspect the model\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "633a4095-70fa-487c-a32d-a6db29a9ac6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from datasets import load_metric\n",
    "# define function to compute metrics\n",
    "def compute_metrics(logits_and_labels):\n",
    "  # metric = load_metric(\"glue\", \"sst2\")\n",
    "  logits, labels = logits_and_labels\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afe54eaa-8444-443a-a8bf-42d2a3061a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# set up trainer to fine-tune the model\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c00982-39b6-41cf-a778-566928e80afb",
   "metadata": {},
   "source": [
    "## Task 3: fine tune the model for 1 epoch!\n",
    "Note that this might take some time. \n",
    "\n",
    "Note that the epoch number was set above in the training arguments. \n",
    "\n",
    "After fine tuning 1 epoch, report the final accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c601e866-9622-4d23-a9e2-f9eabfb97f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7d8bf-dd1e-46fd-9e1e-dc20f1020166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk so that you can load it back later\n",
    "trainer.save_model('my_saved_model')\n",
    "\n",
    "# use this code to massage the labels into something interpretable, NEGATIVE, POSITIVE\n",
    "import json\n",
    "config_path = 'my_saved_model/config.json'\n",
    "with open(config_path) as f:\n",
    "  j = json.load(f)\n",
    "\n",
    "j['id2label'] = {0: 'NEGATIVE', 1: 'POSITIVE'}\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "  json.dump(j, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312c328c-7720-4a05-b964-70d3e0f888c7",
   "metadata": {},
   "source": [
    "## Use the saved model for inference on new sentences\n",
    "\n",
    "Now you can use this newly fine-tuned model to build a `pipeline`, an object in the trnasformers library. The pipeline can be used to make inference on a input sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8cb8561-d01a-41b9-b304-e77362b2c012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9994922876358032}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "new_model = pipeline('text-classification', model='my_saved_model')\n",
    "\n",
    "# test your new pipeline\n",
    "new_model('This movie is great!')\n",
    "\n",
    "# test with more examples \n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f845bce-a396-41b1-8ffe-5e4463755dee",
   "metadata": {},
   "source": [
    "# 4. Evaluate the model: how was the result of the fine-tuning?\n",
    "\n",
    "Once you trained a model, it's always important to show through proper evaluation that this fine-tuned model is indeed better than before fine tuning, or compare this with models fine-tuned by other people.  \n",
    "\n",
    "To use HuggingFace's evaluator, install:\n",
    "`!pip install evaluate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0d4de3d-140c-4b4e-9443-bd3daed7bf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from evaluate import evaluator\n",
    "\n",
    "# first let's load the test portion of the sst2 data\n",
    "test_datasets = load_dataset(\"glue\", \"sst2\", split=\"test\")\n",
    "\n",
    "# let's compare three models and evaluate the against each other. \n",
    "\n",
    "# Model 1: pre-trained model distillBERT as is. Since this is added some new\n",
    "# classifier layers, it is expected to have low performance. \n",
    "# let's load this model again. \n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_distillBERT = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d0700f7-debf-42bf-9228-9293ef14da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: the model you fine tuned. For this one, we already have the pipeline \n",
    "# called new_model, we can use this directly for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10cfd112-ae76-4e57-8467-beb948409585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: the default model for the evaluator if you don't give it any model.\n",
    "# i.e., you would not supply the argument for model_or_pipeline in the following.\n",
    "# In this case, it defaults to a model that was fine-tuned by others. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04261b38-cdc9-4510-82e8-55ddef294076",
   "metadata": {},
   "source": [
    "## Task 4: evaluate the three models!\n",
    "report the results for Model 1, 2 and 3 above on the `test` portion of the `sst2` dataset. What results do you get? Can you think of why? \n",
    "\n",
    "Now try testing the three models on the `validation` portion of the same dataset. Report the results. What do you observe?\n",
    "\n",
    "Hint 1: if you are testing a certain model and got an error about the labels, you might want to use one of the lines that is commented out below and swap it out with another line. \n",
    "\n",
    "Hint 2: if you can't figure out what's wrong about your accuracy, try go back to inspect the data! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "112e2141-eff5-43f9-931c-8890cf3d7956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the evaluator\n",
    "\n",
    "from evaluate import load\n",
    "task_evaluator = evaluator(\"text-classification\")\n",
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=None, #YOUR CODE HERE\n",
    "    data=None, #YOUR CODE HERE\n",
    "    input_column=\"sentence\",\n",
    "    tokenizer=tokenizer,\n",
    "    metric='accuracy',\n",
    "    label_mapping={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "    #label_mapping={\"LABEL_0\": 0.0, \"LABEL_1\": 1.0}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e1bfa-d242-4429-91f1-d8b214a397c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff76b678-81d7-48fe-a9bc-ca46da673443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ad838-ccbb-47b7-b6de-1ea74c2e9c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
